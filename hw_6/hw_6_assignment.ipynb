{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Samuel Watkins, 3032132676"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 6: Homebrew Computer Vision\n",
    "## Due Monday Apr 2, 2018 at 2 PM\n",
    "\n",
    "1. Download the [zip file](https://www.dropbox.com/s/cst9awcjpp08k33/50_categories.tar.gz). Look at some of the images, noting that there are 50 classes in 4244 images (e.g. \"goldfish\", “llama”, “speed-boat”, ...). Caution: it’s a pretty large file (~208M).\n",
    "2. Write a set of methods that takes as input one of these images, and then computes real-numbered features as the return. You should produce at least 15 features.\n",
    "3. Based on the feature set for each image, build a random forest classifier. Produce metrics on your estimated error rates using cross-validation. How much better is this than the expectation with random guessing? What are the 3 most important features?\n",
    "4. Make sure your final classifier can run on a directory of different images, where a call like `run_final_classifier(\"/new/directory/path/\")` on a directory that contains files like `validation1.jpg`, `validation2.jpg`, etc. will produce an output file that looks like:  \n",
    "```\n",
    "filename              predicted_class  \n",
    "``` \n",
    "` `-----------------------------------------------------------------\n",
    "```\n",
    "validation1.jpg       unicorn  \n",
    "validation2.jpg       camel  \n",
    "```\n",
    "\n",
    "    We will have a validation set to test how good your classifier is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to Extract Features from an Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage.feature import corner_harris,peak_local_max,canny,corner_peaks\n",
    "from skimage.segmentation import slic\n",
    "from skimage.color.colorconv import rgb2grey,grey2rgb\n",
    "from skimage.filters import frangi\n",
    "from skimage.transform import rescale\n",
    "from skimage.measure import shannon_entropy\n",
    "from skimage.filters import threshold_otsu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extractImageFeatures(pathToImage,smallImageSize=16000):\n",
    "    imageArray = plt.imread(pathToImage).astype(\"float\")\n",
    "    if len(imageArray.shape)<3:\n",
    "        imageArray = grey2rgb(imageArray)\n",
    "    greyImgArr = rgb2grey(imageArray)\n",
    "    scaleFactor = np.sqrt(smallImageSize/np.prod(greyImgArr.shape))\n",
    "    smallImage = rescale(greyImgArr,scaleFactor,mode = \"constant\")\n",
    "    \n",
    "    # dumb features\n",
    "    imgSize = np.prod(greyImgArr.shape)\n",
    "    imgHeight = greyImgArr.shape[0]\n",
    "    imgWidth = greyImgArr.shape[1]\n",
    "    avgAllChans = np.mean(imageArray)\n",
    "    stdAllChans = np.std(imageArray)\n",
    "    ratioStdAvgAllChans = stdAllChans/avgAllChans\n",
    "    \n",
    "    avgRedChan = np.mean(imageArray[:,:,0])\n",
    "    stdRedChan = np.std(imageArray[:,:,0])\n",
    "    \n",
    "    avgBlueChan = np.mean(imageArray[:,:,1])\n",
    "    stdBlueChan = np.std(imageArray[:,:,1])\n",
    "    \n",
    "    avgGreenChan = np.mean(imageArray[:,:,2])\n",
    "    stdGreenChan = np.std(imageArray[:,:,2])\n",
    "    \n",
    "    ratioRedBlue = avgRedChan/avgBlueChan\n",
    "    ratioBlueGreen = avgBlueChan/avgGreenChan\n",
    "    ratioRedGreen = avgRedChan/avgGreenChan\n",
    "    \n",
    "    ratioStdAvgRedChan = stdRedChan/avgRedChan\n",
    "    ratioStdAvgBlueChan = stdBlueChan/avgBlueChan\n",
    "    ratioStdAvgGreenChan = stdGreenChan/avgGreenChan\n",
    "    \n",
    "    \n",
    "    # \"smart\" features\n",
    "    corners = corner_harris(smallImage)\n",
    "    numCorners = len(corner_peaks(corners))\n",
    "    \n",
    "    peaks = peak_local_max(smallImage)\n",
    "    numPeaks = len(peaks)\n",
    "    \n",
    "    segments = slic(smallImage)\n",
    "    numSegments = np.max(segments)\n",
    "    \n",
    "    edges = canny(frangi(smallImage))\n",
    "    edgeLength = np.sum(edges)\n",
    "\n",
    "    ratioCornersPeak = numCorners/numPeaks\n",
    "    ratioCornersSegments = numCorners/numSegments\n",
    "    ratioCornersEdges = numCorners/edgeLength\n",
    "    ratioPeaksSegments = numPeaks/numSegments\n",
    "    ratioPeaksEdges = numPeaks/edgeLength\n",
    "    ratioSegmentsEdges = numSegments/edgeLength\n",
    "    \n",
    "    shanent = shannon_entropy(imageArray)\n",
    "    \n",
    "    thresh = threshold_otsu(smallImage)\n",
    "    foreground = np.sum(smallImage <=thresh)\n",
    "\n",
    "    features=np.array([imgSize,imgHeight,imgWidth,avgRedChan,stdRedChan,avgBlueChan,stdBlueChan,\n",
    "              avgGreenChan,stdGreenChan,ratioRedBlue,ratioBlueGreen,\n",
    "              ratioRedGreen,ratioStdAvgRedChan,ratioStdAvgBlueChan,ratioStdAvgGreenChan,\n",
    "              avgAllChans,stdAllChans,ratioStdAvgAllChans,\n",
    "              numCorners,numPeaks,numSegments,edgeLength,\n",
    "              ratioCornersPeak,ratioCornersSegments,ratioCornersEdges,ratioPeaksSegments,\n",
    "              ratioPeaksEdges,ratioSegmentsEdges,shanent,thresh,foreground])\n",
    "    \n",
    "    if np.any(np.isnan(features)):\n",
    "        print(pathToImage)\n",
    "    features[np.isnan(features)]=0.0\n",
    "    features[np.isinf(features)]=0.0\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathToImage = \"/home/sam/Documents/watkins-ay250-s2018-hw/hw_6/50_categories/bear/bear_0010.jpg\"\n",
    "imageArray = plt.imread(pathToImage).astype(\"float\")\n",
    "imageArray = grey2rgb(imageArray)\n",
    "greyImgArr = rgb2grey(imageArray)\n",
    "scaleFactor = np.sqrt(16000.0/np.prod(greyImgArr.shape))\n",
    "imgScaled = rescale(greyImgArr,scaleFactor,mode = \"constant\")\n",
    "contedges = canny(frangi(imgScaled))\n",
    "\n",
    "\n",
    "# plt.imshow(contedges)\n",
    "\n",
    "binary = imgScaled <= threshold_otsu(imgScaled)\n",
    "plt.imshow(canny(binary))\n",
    "# timenow=time()\n",
    "# extractImageFeatures(pathToImage);\n",
    "# print(time()-timenow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Features from All Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from multiprocessing import Pool\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in folder 1 of 50 folders...\n",
      "Looking in folder 2 of 50 folders...\n",
      "Looking in folder 3 of 50 folders...\n",
      "Looking in folder 4 of 50 folders...\n",
      "Looking in folder 5 of 50 folders...\n",
      "Looking in folder 6 of 50 folders...\n",
      "Looking in folder 7 of 50 folders...\n",
      "Looking in folder 8 of 50 folders...\n",
      "Looking in folder 9 of 50 folders...\n",
      "Looking in folder 10 of 50 folders...\n",
      "Looking in folder 11 of 50 folders...\n",
      "Looking in folder 12 of 50 folders...\n",
      "Looking in folder 13 of 50 folders...\n",
      "Looking in folder 14 of 50 folders...\n",
      "Looking in folder 15 of 50 folders...\n",
      "Looking in folder 16 of 50 folders...\n",
      "Looking in folder 17 of 50 folders...\n",
      "Looking in folder 18 of 50 folders...\n",
      "Looking in folder 19 of 50 folders...\n",
      "Looking in folder 20 of 50 folders...\n",
      "Looking in folder 21 of 50 folders...\n",
      "Looking in folder 22 of 50 folders...\n",
      "Looking in folder 23 of 50 folders...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sam/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:49: RuntimeWarning: divide by zero encountered in long_scalars\n",
      "/home/sam/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:51: RuntimeWarning: divide by zero encountered in long_scalars\n",
      "/home/sam/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:49: RuntimeWarning: divide by zero encountered in long_scalars\n",
      "/home/sam/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:51: RuntimeWarning: divide by zero encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in folder 24 of 50 folders...\n",
      "Looking in folder 25 of 50 folders...\n",
      "Looking in folder 26 of 50 folders...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sam/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:49: RuntimeWarning: divide by zero encountered in long_scalars\n",
      "/home/sam/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:51: RuntimeWarning: divide by zero encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in folder 27 of 50 folders...\n",
      "Looking in folder 28 of 50 folders...\n",
      "Looking in folder 29 of 50 folders...\n",
      "Looking in folder 30 of 50 folders...\n",
      "Looking in folder 31 of 50 folders...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sam/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:49: RuntimeWarning: divide by zero encountered in long_scalars\n",
      "/home/sam/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:51: RuntimeWarning: divide by zero encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in folder 32 of 50 folders...\n",
      "Looking in folder 33 of 50 folders...\n",
      "Looking in folder 34 of 50 folders...\n",
      "Looking in folder 35 of 50 folders...\n",
      "Looking in folder 36 of 50 folders...\n",
      "Looking in folder 37 of 50 folders...\n",
      "Looking in folder 38 of 50 folders...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sam/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:49: RuntimeWarning: divide by zero encountered in long_scalars\n",
      "/home/sam/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:51: RuntimeWarning: divide by zero encountered in long_scalars\n",
      "/home/sam/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:49: RuntimeWarning: divide by zero encountered in long_scalars\n",
      "/home/sam/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:51: RuntimeWarning: divide by zero encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in folder 39 of 50 folders...\n",
      "Looking in folder 40 of 50 folders...\n",
      "Looking in folder 41 of 50 folders...\n",
      "Looking in folder 42 of 50 folders...\n",
      "Looking in folder 43 of 50 folders...\n",
      "Looking in folder 44 of 50 folders...\n",
      "Looking in folder 45 of 50 folders...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sam/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:49: RuntimeWarning: divide by zero encountered in long_scalars\n",
      "/home/sam/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:51: RuntimeWarning: divide by zero encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in folder 46 of 50 folders...\n",
      "Looking in folder 47 of 50 folders...\n",
      "Looking in folder 48 of 50 folders...\n",
      "Looking in folder 49 of 50 folders...\n",
      "Looking in folder 50 of 50 folders...\n",
      "302.83050751686096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sam/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "pathToImageFolders = \"50_categories/\"\n",
    "eachFolder = glob(pathToImageFolders+\"*/\")\n",
    "train_size = 0.5 # ratio of training dataset to total dataset\n",
    "X = list()\n",
    "Y = list()\n",
    "\n",
    "# open up 16 processes to extract features in parallel\n",
    "num_processes = 16\n",
    "pool = Pool(processes=num_processes)\n",
    "\n",
    "starttime = time()\n",
    "for iFolder,folder in enumerate(eachFolder):\n",
    "    print(f\"Looking in folder {iFolder+1} of {len(eachFolder)} folders...\")\n",
    "    filesInFolder = glob(folder+\"*.jpg\")\n",
    "    parallelFeatures = pool.map(extractImageFeatures,filesInFolder)\n",
    "    X.append(np.vstack(parallelFeatures))\n",
    "    Y.append(np.repeat(folder[len(pathToImageFolders):-1],len(filesInFolder)))\n",
    "\n",
    "print(time()-starttime)\n",
    "        \n",
    "pool.terminate()\n",
    "del pool\n",
    "\n",
    "X = np.vstack(X)\n",
    "Y = np.concatenate(Y)\n",
    "        \n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,train_size=train_size,stratify=Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build A Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "randforclf = RandomForestClassifier(n_estimators=50)\n",
    "\n",
    "randforclf.fit(X_train,Y_train)\n",
    "\n",
    "pred_rf = randforclf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.29688972667295005\n",
      "Accuracy from cross-validation: 0.29979769744902585 (+/- 0.012319788085494752)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Score: {metrics.accuracy_score(Y_test,pred_rf)}\")\n",
    "scores = cross_val_score(randforclf,X,Y,cv=5,groups=Y)\n",
    "print(f\"Accuracy from cross-validation: {np.mean(scores)} (+/- {np.std(scores)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05261205, 0.06150218, 0.05622375, 0.02648454, 0.02782996,\n",
       "       0.0284188 , 0.02592573, 0.0257615 , 0.02967849, 0.03422208,\n",
       "       0.03410235, 0.03207322, 0.02728713, 0.02694469, 0.02831861,\n",
       "       0.02715394, 0.02641158, 0.02404731, 0.024617  , 0.0369651 ,\n",
       "       0.03026014, 0.03024907, 0.02895527, 0.02803103, 0.02707987,\n",
       "       0.03065793, 0.03383542, 0.02991957, 0.04433411, 0.02996621,\n",
       "       0.0301314 ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randforclf.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare to Random Guessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dummyclf = DummyClassifier(strategy=\"prior\",random_state=42)\n",
    "\n",
    "dummyclf.fit(X_train,Y_train)\n",
    "\n",
    "dummypred_rf = dummyclf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12582469368520263\n",
      "0.12562240614744166 (+/- 0.0018389617183439597)\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(Y_test,dummypred_rf))\n",
    "scores = cross_val_score(dummyclf,X,Y,cv=5,groups=Y)\n",
    "print(f\"{np.mean(scores)} (+/- {np.std(scores)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
